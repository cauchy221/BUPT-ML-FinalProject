{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据观察和预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原始数据放在 `BUPT-ML-FinalProject\\data\\release` 中，其中 `train.csv` 为训练集，`test.csv` 为测试集，`sample.csv` 为提交样例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据观察"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 直观上看，数据量很小，`train.csv` 和 `test.csv` 分别只有 981 和 979 条样本\n",
    "2. 训练集的格式为 `sentiment,text`；测试集的格式为 `id,text`\n",
    "3. 官方没有给 `val.csv`，这意味着我们需要按比例随机划分出验证集\n",
    "4. 训练集中情感 1-5 的比例并不均衡，直观上大致符合中性情感最多，极端情感较少的规律，需要进一步根据统计情况分析\n",
    "5. 数据中包含一些需要清洗掉的部分，比如\n",
    "    - url 链接\n",
    "    - `@` 引导的用户名\n",
    "    - `\"\"` 双引号\n",
    "    - `Ì¢‰âÂ‰ã¢` 等乱码\n",
    "    - `&amp;` 在 HTML 中表示 `&`，考虑暂时替换成单词 `and`（或许可能还有其它 HTML 编码）\n",
    "    - `#xxx` 表示话题，包含有信息，不能直接删除，只能删掉符号\n",
    "6. 为了提高模型的正确率，我们在网上搜索到了 `self-driving car sentiment` 数据集，并存放在 `data\\release\\origin.csv` 中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本数据清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_train = ['sentiment','text']\n",
    "cols_test = ['id','text']\n",
    "df_train_small = pd.read_csv('./data/release/train.csv', header=0)\n",
    "df_test_small = pd.read_csv('./data/release/test.csv', header=0)\n",
    "df_origin = pd.read_csv('./data/release/origin.csv', header=0)\n",
    "df_origin = df_origin[['sentiment', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(text):\n",
    "    text = re.sub(r'https?://[A-Za-z0-9./]+', '', str(text))  # url\n",
    "    text = text.replace('Ì¢‰âÂ‰ã¢', ' ') # 一种特殊的乱码，基本出现在 we re、it s 这种的中间，替换为空格\n",
    "    text = re.sub(r'[^a-z_A-Z0-9-\\.!@#\\$%\\\\\\^&\\*\\)\\(\\+=\\{\\}\\[\\]\\/\\\"\\,\\'<>~\\·`\\?:;|\\s]', '', text)  # 非字母、数字、符号的其他乱码\n",
    "    text = re.sub(r'[MR]T ?@[a-z]+', '', text) # RT @username、MT @username是 twitter 网站一种特殊标记\n",
    "    text = re.sub(r'@[A-Za-z0-9_]+', '', text)  # @username\n",
    "    text = re.sub(r'[0-9]+:[0-9]+ [AP]M', '', text) # 时间戳\n",
    "    text = re.sub(r'\\d+', ' ', text) # 去除数字\n",
    "    text = re.sub(r'R[Ee]:', '', text) # 表示回复消息的 RE\n",
    "    text = text.replace(',', '').replace(r'\"', '').replace('#', '').replace('&', 'and')  # , and \"\" and # and &\n",
    "    text = BeautifulSoup(text, 'lxml').get_text()  # html编码\n",
    "    text = re.sub(r'[_\\-@%#\\$\\^\\*\\)\\(\\+=\\{\\}\\[\\]\\/\\\"<>~`:;|]', ' ', text)  # 剩下的一些无用符号\n",
    "    text = text.lower() # 转小写\n",
    "    text = re.sub(r'^ +', '', text) # 开头的空格去除\n",
    "    text = re.sub(r' +$', '', text) # 结尾的空格去除\n",
    "    text = re.sub(r' +', ' ', text) # 多个空格替换成一个\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 小数据集\n",
    "df_train_small['text'] = df_train_small['text'].map(data_cleaning)\n",
    "df_test_small['text'] = df_test_small['text'].map(data_cleaning)\n",
    "df_train_small.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 大数据集\n",
    "df_origin = df_origin[df_origin['sentiment']!='not_relevant'] # 删除 not_relevant 行\n",
    "df_origin['sentiment'] = df_origin['sentiment'].astype(np.int32) # 将 sentiment 列类型改为 int32，方便后续 split 操作\n",
    "df_origin['text'] = df_origin['text'].map(data_cleaning)\n",
    "df_origin.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可视化与进一步处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# 忽略所有警告（比如，绘制箱型图有警告）\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 观察各情感分类数据量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 小数据集 `data/preprocessed/small` 分类统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,2,3,4,5]\n",
    "y = []\n",
    "for i in range(1,6):\n",
    "    y.append(df_train_small[df_train_small['sentiment']==i]['sentiment'].count())\n",
    "\n",
    "plt.bar(x, y)\n",
    "plt.xlabel('sentiment')\n",
    "plt.ylabel('count')\n",
    "for a,b,i in zip(x, y, range(len(x))):\n",
    "    plt.text(a, b + 0.01, y[i], ha='center', fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 大数据集 `data/preprocessed/big` 分类统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = []\n",
    "for i in range(1,6):\n",
    "    y2.append(df_origin[df_origin['sentiment']==i]['sentiment'].count())\n",
    "\n",
    "plt.bar(x, y2)\n",
    "plt.xlabel('sentiment')\n",
    "plt.ylabel('count')\n",
    "for a,b,i in zip(x, y2, range(len(x))):\n",
    "    plt.text(a, b + 0.01, y2[i], ha='center', fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分布直方图查看 sentiment 分布和 text 长度分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "\n",
    "plt.subplot(2, 4, 1)\n",
    "sns.distplot(df_tmp['sentiment'], fit=stats.norm)\n",
    "\n",
    "plt.subplot(2, 4, 2)\n",
    "stats.probplot(df_tmp['sentiment'], plot=plt)\n",
    "\n",
    "plt.subplot(2, 4, 3)\n",
    "sns.distplot(df_tmp['len'], fit=stats.norm)\n",
    "\n",
    "plt.subplot(2, 4, 4)\n",
    "stats.probplot(df_tmp['len'], plot=plt)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_small(val_ratio=0.2, split_seed=4567):\n",
    "    df_test_small.to_csv('./data/preprocessed/small_fix/test.csv', index=False)\n",
    "    \n",
    "    dir_train = './data/preprocessed/small_fix/train.csv'\n",
    "    dir_val = './data/preprocessed/small_fix/dev.csv'\n",
    "\n",
    "    headers = pd.DataFrame(columns=cols_train)\n",
    "    headers.to_csv(dir_train, index=False)\n",
    "    headers.to_csv(dir_val, index=False)\n",
    "    \n",
    "    for i in range(1, 6):\n",
    "        df_train_split, df_val_split = train_test_split(df_train_small[df_train_small['sentiment']==i], test_size=val_ratio, random_state=split_seed)\n",
    "        df_train_split.to_csv(dir_train, index=False, mode='a', header=None)\n",
    "        df_val_split.to_csv(dir_val, index=False, mode='a', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_big_with_test(first_ratio=0.2, second_ratio=0.5, split_seed=1234):\n",
    "    dir_train = './data/preprocessed/big/train.csv'\n",
    "    dir_val = './data/preprocessed/big/dev.csv'\n",
    "    dir_test = './data/preprocessed/big/test.csv'\n",
    "\n",
    "    headers = pd.DataFrame(columns=cols_train)\n",
    "    headers.to_csv(dir_train, index=False)\n",
    "    headers.to_csv(dir_val, index=False)\n",
    "    headers.to_csv(dir_test, index=False)\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        df_train_split, df_other_split = train_test_split(df_origin[df_origin['sentiment']==i], test_size=first_ratio, random_state=split_seed)\n",
    "        df_train_split.to_csv(dir_train, index=False, mode='a', header=None)\n",
    "        \n",
    "        df_val_split, df_test_split = train_test_split(df_other_split[df_other_split['sentiment']==i], test_size=second_ratio, random_state=split_seed)\n",
    "        df_val_split.to_csv(dir_val, index=False, mode='a', header=None)\n",
    "        df_test_split.to_csv(dir_test, index=False, mode='a', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_big_no_test(sample_num=6800, val_ratio=0.1, split_seed=1234):\n",
    "    dir_train = './data/preprocessed/big/train.csv'\n",
    "    dir_val = './data/preprocessed/big/dev.csv'\n",
    "\n",
    "    headers = pd.DataFrame(columns=cols_train)\n",
    "    headers.to_csv(dir_train, index=False)\n",
    "    headers.to_csv(dir_val, index=False)\n",
    "\n",
    "    df_sample = df_origin.copy(deep=True)\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        df_train_split, df_val_split = train_test_split(df_sample[df_sample['sentiment']==i], test_size=val_ratio, random_state=split_seed)\n",
    "        df_train_split.to_csv(dir_train, index=False, mode='a', header=None)\n",
    "        df_val_split.to_csv(dir_val, index=False, mode='a', header=None)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_small()\n",
    "# split_big_with_test()\n",
    "# split_big_no_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e80f2c8833cedb7126ef3cddc2d95ca2b11987ff57ef56b65aa4ced506198002"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
