{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据观察和预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的数据放在 `BUPT-ML-FinalProject\\release\\data` 中，其中 `train.csv` 为训练集，`test.csv` 为测试集，`sample.csv` 为提交样例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据观察"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 直观上看，数据量很小，`train.csv` 和 `test.csv` 分别只有 981 和 979 条样本\n",
    "2. 训练集的格式为 `sentiment,text`；测试集的格式为 `id,text`\n",
    "3. 官方没有给 `val.csv`，这意味着我们需要按比例随机划分出验证集\n",
    "4. 训练集中情感 1-5 的比例并不均衡，直观上大致符合中性情感最多，极端情感较少的规律，需要进一步根据统计情况分析\n",
    "5. 数据中包含一些需要清洗掉的部分，比如\n",
    "    - url 链接\n",
    "    - `@` 引导的用户名\n",
    "    - `\"\"` 双引号\n",
    "    - `Ì¢‰âÂ‰ã¢` 等乱码\n",
    "    - `&amp;` 在 HTML 中表示 `&`，考虑暂时替换成单词 `and`（或许可能还有其它 HTML 编码）\n",
    "    - `#xxx` 表示话题，包含有信息，不能直接删除，只能删掉符号"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_train = ['sentiment','text']\n",
    "cols_test = ['id','text']\n",
    "df_train_small = pd.read_csv('./data/release/train.csv', header=0)\n",
    "df_test_small = pd.read_csv('./data/release/test.csv', header=0)\n",
    "df_origin = pd.read_csv('./data/release/origin.csv', header=0)\n",
    "df_origin = df_origin[['sentiment', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(text):\n",
    "    text = re.sub(r'https?://[A-Za-z0-9./]+', '', str(text))  # url\n",
    "    text = text.replace('Ì¢‰âÂ‰ã¢', ' ') # 一种特殊的乱码，基本出现在 we re、it s 这种的中间，替换为空格\n",
    "    text = re.sub(r'[^a-z_A-Z0-9-\\.!@#\\$%\\\\\\^&\\*\\)\\(\\+=\\{\\}\\[\\]\\/\\\"\\,\\'<>~\\·`\\?:;|\\s]', '', text)  # 非字母、数字、符号的其他乱码\n",
    "    text = re.sub(r'[MR]T ?@[a-z]+', '', text) # RT @username、MT @username是 twitter 网站一种特殊标记\n",
    "    text = re.sub(r'@[A-Za-z0-9_]+', '', text)  # @username\n",
    "    text = re.sub(r'[0-9]+:[0-9]+ [AP]M', '', text) # 时间戳\n",
    "    text = re.sub(r'\\d+', ' ', text) # 去除数字\n",
    "    text = re.sub(r'R[Ee]:', '', text) # 表示回复消息的 RE\n",
    "    text = text.replace(',', '').replace(r'\"', '').replace('#', '').replace('&', 'and')  # , and \"\" and # and &\n",
    "    text = BeautifulSoup(text, 'lxml').get_text()  # html编码\n",
    "    text = re.sub(r'[_\\-@%#\\$\\^\\*\\)\\(\\+=\\{\\}\\[\\]\\/\\\"<>~`:;|]', ' ', text)  # 剩下的一些无用符号\n",
    "    text = text.lower() # 转小写\n",
    "    text = re.sub(r'^ +', '', text) # 开头的空格去除\n",
    "    text = re.sub(r' +$', '', text) # 结尾的空格去除\n",
    "    text = re.sub(r' +', ' ', text) # 多个空格替换成一个\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\programs\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df_train_small['text'] = df_train_small['text'].map(data_cleaning)\n",
    "df_test_small['text'] = df_test_small['text'].map(data_cleaning)\n",
    "\n",
    "df_origin = df_origin[df_origin['sentiment']!='not_relevant'] # 删除 not_relevant 行\n",
    "df_origin['sentiment'] = df_origin['sentiment'].astype(np.int32) # 将 sentiment 列类型改为 int32，方便后续 split 操作\n",
    "df_origin['text'] = df_origin['text'].map(data_cleaning)\n",
    "\n",
    "# df_origin.head()\n",
    "\n",
    "# label minus one to fit the model\n",
    "# df_train['sentiment'] = df_train['sentiment'].map(lambda x: x-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_small(val_ratio=0.2, split_seed=1234):\n",
    "    df_test_small.to_csv('./data/preprocessed/small/test.csv', index=False)\n",
    "    \n",
    "    dir_train = './data/preprocessed/small/train.csv'\n",
    "    dir_val = './data/preprocessed/small/dev.csv'\n",
    "\n",
    "    headers = pd.DataFrame(columns=cols_train)\n",
    "    headers.to_csv(dir_train, index=False)\n",
    "    headers.to_csv(dir_val, index=False)\n",
    "    \n",
    "    for i in range(1, 6):\n",
    "        df_train_split, df_val_split = train_test_split(df_train_small[df_train_small['sentiment']==i], test_size=val_ratio, random_state=split_seed)\n",
    "        df_train_split.to_csv(dir_train, index=False, mode='a', header=None)\n",
    "        df_val_split.to_csv(dir_val, index=False, mode='a', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_big_with_test(first_ratio=0.2, second_ratio=0.5, split_seed=1234):\n",
    "    dir_train = './data/preprocessed/big/train.csv'\n",
    "    dir_val = './data/preprocessed/big/dev.csv'\n",
    "    dir_test = './data/preprocessed/big/test.csv'\n",
    "\n",
    "    headers = pd.DataFrame(columns=cols_train)\n",
    "    headers.to_csv(dir_train, index=False)\n",
    "    headers.to_csv(dir_val, index=False)\n",
    "    headers.to_csv(dir_test, index=False)\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        # print(df_origin.head())\n",
    "        # print(df_origin.info())\n",
    "        df_train_split, df_other_split = train_test_split(df_origin[df_origin['sentiment']==i], test_size=first_ratio, random_state=split_seed)\n",
    "        df_train_split.to_csv(dir_train, index=False, mode='a', header=None)\n",
    "        \n",
    "        df_val_split, df_test_split = train_test_split(df_other_split[df_other_split['sentiment']==i], test_size=second_ratio, random_state=split_seed)\n",
    "        df_val_split.to_csv(dir_val, index=False, mode='a', header=None)\n",
    "        df_test_split.to_csv(dir_test, index=False, mode='a', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_big_no_test(sample_num=6800, val_ratio=0.1, split_seed=1234):\n",
    "    dir_train = './data/preprocessed/big/train.csv'\n",
    "    dir_val = './data/preprocessed/big/dev.csv'\n",
    "\n",
    "    headers = pd.DataFrame(columns=cols_train)\n",
    "    headers.to_csv(dir_train, index=False)\n",
    "    headers.to_csv(dir_val, index=False)\n",
    "\n",
    "    df_sample = df_origin.sample(n=sample_num, axis=0, random_state=split_seed)\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        df_train_split, df_val_split = train_test_split(df_sample[df_sample['sentiment']==i], test_size=val_ratio, random_state=split_seed)\n",
    "        df_train_split.to_csv(dir_train, index=False, mode='a', header=None)\n",
    "        df_val_split.to_csv(dir_val, index=False, mode='a', header=None)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_small()\n",
    "# split_big_with_test()\n",
    "split_big_no_test()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "af5a88a29c9657f0761de652cb405995f678c86c46afeea753dc663c103ad1ae"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
